{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Running benchmarks...\n",
      "Forward called with args: () and kwargs: {'input_ids': tensor([[  101,  1996, 14523,  2058, 13971,  3899, 14523,  2829,  3899,  4419,\n",
      "          4419,   102],\n",
      "        [  101,  3899,  2829,  2829,  1996,  3899,  3899,  1996,  2058,  4419,\n",
      "          2829,   102],\n",
      "        [  101,  2829,  2829, 13971,  1996, 13971,  4248,  4419,  3899,  4248,\n",
      "          2829,   102],\n",
      "        [  101,  2829, 14523, 13971,  2829,  4248,  3899,  3899,  1996,  2829,\n",
      "         14523,   102],\n",
      "        [  101,  1996,  3899,  4248, 14523, 14523,  2058,  4248,  1996,  2829,\n",
      "          4248,   102],\n",
      "        [  101,  3899,  4419,  4248, 13971,  4248,  4419,  4419,  1996, 14523,\n",
      "         13971,   102],\n",
      "        [  101,  2058,  2829, 14523,  2058,  2058,  4248, 13971,  3899,  4248,\n",
      "          4419,   102],\n",
      "        [  101,  1996,  4419, 14523,  2058,  1996, 14523,  1996,  2829,  3899,\n",
      "          2829,   102],\n",
      "        [  101, 14523,  2829,  2058,  4419,  2829,  1996,  4248, 13971,  1996,\n",
      "          3899,   102],\n",
      "        [  101,  2829, 14523,  2829,  4248,  3899,  2058,  2829,  3899, 13971,\n",
      "          4248,   102],\n",
      "        [  101,  4419, 14523,  2829,  2829,  4248,  3899,  3899,  2829, 14523,\n",
      "          2829,   102],\n",
      "        [  101,  1996, 14523, 13971,  1996,  4419,  2058,  1996,  1996,  1996,\n",
      "          2058,   102],\n",
      "        [  101,  3899,  2058, 14523,  1996,  4419,  4248, 14523,  2829,  4419,\n",
      "         14523,   102],\n",
      "        [  101,  4419,  4419,  2829,  2058, 13971,  2058,  4248,  2829, 13971,\n",
      "         14523,   102],\n",
      "        [  101,  2058, 14523,  4419,  1996,  2058,  4419, 14523,  3899,  4248,\n",
      "          2058,   102],\n",
      "        [  101, 13971,  3899,  4248,  3899,  1996,  4419,  4248,  1996,  3899,\n",
      "         14523,   102],\n",
      "        [  101,  2058,  2829,  4248,  4248,  2058,  2829,  1996,  2829,  4419,\n",
      "          2829,   102],\n",
      "        [  101,  4248,  2058,  2829, 13971,  2829,  3899,  4419, 13971,  4248,\n",
      "          2058,   102],\n",
      "        [  101,  3899,  4419,  1996,  1996,  4419,  4419,  1996,  2058,  1996,\n",
      "          2058,   102],\n",
      "        [  101,  2058, 14523, 13971,  2058,  2058,  2058,  4248, 14523,  1996,\n",
      "          1996,   102],\n",
      "        [  101, 14523,  4419,  1996,  1996,  2058,  4419,  3899,  1996, 13971,\n",
      "          3899,   102],\n",
      "        [  101,  3899,  2829,  3899,  3899, 13971,  4419,  4248,  3899, 14523,\n",
      "         14523,   102],\n",
      "        [  101,  4419,  1996,  4248, 13971,  2829,  4419,  2058,  4248,  4419,\n",
      "          4419,   102],\n",
      "        [  101, 14523,  4419,  3899,  1996,  4248,  1996,  1996,  1996,  4419,\n",
      "          4248,   102],\n",
      "        [  101,  2058,  2058, 14523, 14523,  4419,  1996,  2058,  2829,  2058,\n",
      "          4248,   102],\n",
      "        [  101,  4419,  4419,  2058,  4419,  3899, 13971,  1996,  2829,  4419,\n",
      "          2829,   102],\n",
      "        [  101,  4248,  3899,  1996,  2829,  1996,  1996,  4248, 14523,  4419,\n",
      "          2058,   102],\n",
      "        [  101,  3899,  3899,  3899,  4419,  4419,  2829, 13971,  2829,  3899,\n",
      "          1996,   102],\n",
      "        [  101,  1996,  4248,  3899,  4248,  4419, 13971,  2058,  4419, 13971,\n",
      "          2058,   102],\n",
      "        [  101,  4248,  4248, 14523,  2058,  4419,  1996,  4248, 13971,  3899,\n",
      "          1996,   102],\n",
      "        [  101,  3899,  4419,  4419,  1996, 13971,  1996, 14523,  2058,  3899,\n",
      "         13971,   102],\n",
      "        [  101,  3899,  4419,  1996, 14523,  2058,  4419,  2058,  1996,  4419,\n",
      "          2058,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Forward called with args: () and kwargs: {'input_ids': tensor([[  101,  1996, 14523,  2058, 13971,  3899, 14523,  2829,  3899,  4419,\n",
      "          4419,   102],\n",
      "        [  101,  3899,  2829,  2829,  1996,  3899,  3899,  1996,  2058,  4419,\n",
      "          2829,   102],\n",
      "        [  101,  2829,  2829, 13971,  1996, 13971,  4248,  4419,  3899,  4248,\n",
      "          2829,   102],\n",
      "        [  101,  2829, 14523, 13971,  2829,  4248,  3899,  3899,  1996,  2829,\n",
      "         14523,   102],\n",
      "        [  101,  1996,  3899,  4248, 14523, 14523,  2058,  4248,  1996,  2829,\n",
      "          4248,   102],\n",
      "        [  101,  3899,  4419,  4248, 13971,  4248,  4419,  4419,  1996, 14523,\n",
      "         13971,   102],\n",
      "        [  101,  2058,  2829, 14523,  2058,  2058,  4248, 13971,  3899,  4248,\n",
      "          4419,   102],\n",
      "        [  101,  1996,  4419, 14523,  2058,  1996, 14523,  1996,  2829,  3899,\n",
      "          2829,   102],\n",
      "        [  101, 14523,  2829,  2058,  4419,  2829,  1996,  4248, 13971,  1996,\n",
      "          3899,   102],\n",
      "        [  101,  2829, 14523,  2829,  4248,  3899,  2058,  2829,  3899, 13971,\n",
      "          4248,   102],\n",
      "        [  101,  4419, 14523,  2829,  2829,  4248,  3899,  3899,  2829, 14523,\n",
      "          2829,   102],\n",
      "        [  101,  1996, 14523, 13971,  1996,  4419,  2058,  1996,  1996,  1996,\n",
      "          2058,   102],\n",
      "        [  101,  3899,  2058, 14523,  1996,  4419,  4248, 14523,  2829,  4419,\n",
      "         14523,   102],\n",
      "        [  101,  4419,  4419,  2829,  2058, 13971,  2058,  4248,  2829, 13971,\n",
      "         14523,   102],\n",
      "        [  101,  2058, 14523,  4419,  1996,  2058,  4419, 14523,  3899,  4248,\n",
      "          2058,   102],\n",
      "        [  101, 13971,  3899,  4248,  3899,  1996,  4419,  4248,  1996,  3899,\n",
      "         14523,   102],\n",
      "        [  101,  2058,  2829,  4248,  4248,  2058,  2829,  1996,  2829,  4419,\n",
      "          2829,   102],\n",
      "        [  101,  4248,  2058,  2829, 13971,  2829,  3899,  4419, 13971,  4248,\n",
      "          2058,   102],\n",
      "        [  101,  3899,  4419,  1996,  1996,  4419,  4419,  1996,  2058,  1996,\n",
      "          2058,   102],\n",
      "        [  101,  2058, 14523, 13971,  2058,  2058,  2058,  4248, 14523,  1996,\n",
      "          1996,   102],\n",
      "        [  101, 14523,  4419,  1996,  1996,  2058,  4419,  3899,  1996, 13971,\n",
      "          3899,   102],\n",
      "        [  101,  3899,  2829,  3899,  3899, 13971,  4419,  4248,  3899, 14523,\n",
      "         14523,   102],\n",
      "        [  101,  4419,  1996,  4248, 13971,  2829,  4419,  2058,  4248,  4419,\n",
      "          4419,   102],\n",
      "        [  101, 14523,  4419,  3899,  1996,  4248,  1996,  1996,  1996,  4419,\n",
      "          4248,   102],\n",
      "        [  101,  2058,  2058, 14523, 14523,  4419,  1996,  2058,  2829,  2058,\n",
      "          4248,   102],\n",
      "        [  101,  4419,  4419,  2058,  4419,  3899, 13971,  1996,  2829,  4419,\n",
      "          2829,   102],\n",
      "        [  101,  4248,  3899,  1996,  2829,  1996,  1996,  4248, 14523,  4419,\n",
      "          2058,   102],\n",
      "        [  101,  3899,  3899,  3899,  4419,  4419,  2829, 13971,  2829,  3899,\n",
      "          1996,   102],\n",
      "        [  101,  1996,  4248,  3899,  4248,  4419, 13971,  2058,  4419, 13971,\n",
      "          2058,   102],\n",
      "        [  101,  4248,  4248, 14523,  2058,  4419,  1996,  4248, 13971,  3899,\n",
      "          1996,   102],\n",
      "        [  101,  3899,  4419,  4419,  1996, 13971,  1996, 14523,  2058,  3899,\n",
      "         13971,   102],\n",
      "        [  101,  3899,  4419,  1996, 14523,  2058,  4419,  2058,  1996,  4419,\n",
      "          2058,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Dynamic Batching Benchmarking\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import batch\n",
    "import torch\n",
    "\n",
    "# Load a pretrained model (e.g., BERT)\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Helper function to generate random text\n",
    "def generate_random_text(num_words=10):\n",
    "    words = np.random.choice(['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog'], size=num_words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Patch the model's forward method for dynamic batching\n",
    "def forward_wrapper(*args, **kwargs):\n",
    "    print(\"Forward called with args:\", args, \"and kwargs:\", kwargs)\n",
    "    with torch.no_grad():\n",
    "        return model.forward(*args, **kwargs)\n",
    "\n",
    "model.forward = batch.dynamically(forward_wrapper)\n",
    "\n",
    "# Simple batching function\n",
    "def simple_batch_inference(texts, batch_size=32):\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = forward_wrapper(**inputs)\n",
    "        results.extend(outputs['last_hidden_state'][:, 0, :].numpy())\n",
    "    return results\n",
    "\n",
    "# Dynamic batching function\n",
    "def dynamic_batch_inference(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs[\"last_hidden_state\"][:, 0, :].numpy()\n",
    "\n",
    "# Benchmark: Simple vs. Dynamic Batching (without threads or asyncio)\n",
    "def benchmark_simple_vs_dynamic(num_requests=1000):\n",
    "    texts = [generate_random_text() for _ in range(num_requests)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    simple_results = simple_batch_inference(texts)\n",
    "    simple_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    dynamic_results = dynamic_batch_inference(texts)\n",
    "    dynamic_time = time.time() - start_time\n",
    "    \n",
    "    return simple_time, dynamic_time\n",
    "\n",
    "# Benchmark: Simple vs. Dynamic Batching with Threads\n",
    "def benchmark_threaded(num_requests=1000, num_threads=4):\n",
    "    texts = [generate_random_text() for _ in range(num_requests)]\n",
    "    \n",
    "    def process_simple(text):\n",
    "        return simple_batch_inference([text])[0]\n",
    "    \n",
    "    def process_dynamic(text):\n",
    "        return dynamic_batch_inference([text])[0]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        simple_results = list(executor.map(process_simple, texts))\n",
    "    simple_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        dynamic_results = list(executor.map(process_dynamic, texts))\n",
    "    dynamic_time = time.time() - start_time\n",
    "    \n",
    "    return simple_time, dynamic_time\n",
    "\n",
    "# Benchmark: Simple vs. Dynamic Batching with Asyncio\n",
    "async def benchmark_asyncio(num_requests=1000):\n",
    "    texts = [generate_random_text() for _ in range(num_requests)]\n",
    "    \n",
    "    async def process_simple(text):\n",
    "        return simple_batch_inference([text])[0]\n",
    "    \n",
    "    async def process_dynamic(text):\n",
    "        return dynamic_batch_inference([text])[0]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    simple_results = await asyncio.gather(*[process_simple(text) for text in texts])\n",
    "    simple_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    dynamic_results = await asyncio.gather(*[process_dynamic(text) for text in texts])\n",
    "    dynamic_time = time.time() - start_time\n",
    "    \n",
    "    return simple_time, dynamic_time\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Running benchmarks...\")\n",
    "\n",
    "simple_time, dynamic_time = benchmark_simple_vs_dynamic()\n",
    "print(f\"Simple vs. Dynamic (no threads/asyncio): {simple_time:.2f}s vs {dynamic_time:.2f}s\")\n",
    "\n",
    "simple_time_threaded, dynamic_time_threaded = benchmark_threaded()\n",
    "print(f\"Simple vs. Dynamic (threaded): {simple_time_threaded:.2f}s vs {dynamic_time_threaded:.2f}s\")\n",
    "\n",
    "simple_time_asyncio, dynamic_time_asyncio = asyncio.run(benchmark_asyncio())\n",
    "print(f\"Simple vs. Dynamic (asyncio): {simple_time_asyncio:.2f}s vs {dynamic_time_asyncio:.2f}s\")\n",
    "\n",
    "# Prepare data for visualization\n",
    "data = {\n",
    "    'Method': ['Simple', 'Dynamic'] * 3,\n",
    "    'Scenario': ['No Threads/Asyncio'] * 2 + ['Threaded'] * 2 + ['Asyncio'] * 2,\n",
    "    'Time (s)': [simple_time, dynamic_time,\n",
    "                 simple_time_threaded, dynamic_time_threaded,\n",
    "                 simple_time_asyncio, dynamic_time_asyncio]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Scenario', y='Time (s)', hue='Method', data=df)\n",
    "plt.title('Benchmarking Results: Simple vs. Dynamic Batching')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.legend(title='Batching Method')\n",
    "plt.show()\n",
    "\n",
    "# Calculate speedup\n",
    "speedup_no_threads = simple_time / dynamic_time\n",
    "speedup_threaded = simple_time_threaded / dynamic_time_threaded\n",
    "speedup_asyncio = simple_time_asyncio / dynamic_time_asyncio\n",
    "\n",
    "print(f\"\\nSpeedup (Dynamic vs. Simple):\")\n",
    "print(f\"No Threads/Asyncio: {speedup_no_threads:.2f}x\")\n",
    "print(f\"Threaded: {speedup_threaded:.2f}x\")\n",
    "print(f\"Asyncio: {speedup_asyncio:.2f}x\")\n",
    "\n",
    "# Create speedup bar plot\n",
    "speedup_data = {\n",
    "    'Scenario': ['No Threads/Asyncio', 'Threaded', 'Asyncio'],\n",
    "    'Speedup': [speedup_no_threads, speedup_threaded, speedup_asyncio]\n",
    "}\n",
    "\n",
    "df_speedup = pd.DataFrame(speedup_data)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='Scenario', y='Speedup', data=df_speedup)\n",
    "plt.title('Speedup of Dynamic Batching vs. Simple Batching')\n",
    "plt.ylabel('Speedup Factor')\n",
    "plt.show()\n",
    "\n",
    "# Restore original forward method\n",
    "model.forward = original_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
